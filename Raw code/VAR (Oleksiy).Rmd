---
title: "TS Zillow Team Project (VAR CV model)"
author: "Oleksiy Anokhin"
date: "10/29/2020"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 5
---

### Intro

#### Packages

```{r, message = FALSE, warning = FALSE}
# Install packages
library(tidyverse) # for general analysis
library(fpp)
# library(fpp2)
library(fpp3) # for predictions 
# Important notice: the latest package here is fpp3, but it does not work well sometimes with 'ts' data. 
# https://stackoverflow.com/questions/64348121/error-objects-of-type-ts-not-supported-by-autoplot-cannot-knit-rmarkdown-pack
# For example, I cannot use 'autoplot()' function for such data
# Hence, I will use fpp2 for this assignment
library(ggthemes) # for beautiful themes
# library(tinytex) # for pdf documents
# library(highcharter) # for beautiful charts
library(TSA) # for TS analysis
# library(readxl) # for xls files
library(kableExtra) # for beautiful tables
library(lubridate) # for time data
library(tsibble) # for tsiblle data format
# library(plotly) # for interactive charts
library(caret) # for modeling
library(prophet) # for TS analysis
library(lubridate)
library(zoo)
library(vars)
library(stats)
# library(forecast)
library(urca)
library(stats)

# Set a theme
theme_set(theme_minimal())

# Remove scientific notations for prices
options(scipen = 999)
``` 

#### Import data

This dataset was aggregated from two different datasets - Median Sale Price in Chicago (Zillow) and Average Mortgage Rate in Chicago area (Federal data).

```{r, message = FALSE, warning = FALSE}
# Read data
chicago_data <- read_csv("VAR data.csv")

# Print
head(chicago_data)
```
#### EDA

```{r, message = FALSE, warning = FALSE}
# Split data into train and test
Median_price_ts <- ts(chicago_data$Median_price, start = c(2008, 2), frequency = 12)

# Print 
Median_price_ts

# Plot
autoplot(Median_price_ts, color = "#1277e1", size = 1) +
         labs(title = "Median Sale Price in Chicago Metro Area", subtitle = "February 2008 - August 2020") +
         xlab("Monthly data by year") + ylab("Median Sale Price")

```

What can we see here?

- Decreasing and increasing trends
- Strong seasonality
- Data fluctuation changes in magnitude

```{r, message = FALSE, warning = FALSE}
# Split data into train and test
Average_rate_ts <- ts(chicago_data$Average_rate, start = c(2008, 2), frequency = 12)

# Print 
Average_rate_ts

# Plot
autoplot(Average_rate_ts, color = "#1277e1", size = 1) +
         labs(title = "Average Mortgage Rate in Chicago Metro Area", subtitle = "February 2008 - August 2020") +
         xlab("Monthly data by year") + ylab("Average rate")
```



#### Split into train and test data

```{r, message = FALSE, warning = FALSE}
# Split train and test data for Median price
Median_price_ts_train <- window(Median_price_ts, end = c(2018, 12), frequency = 12)
Median_price_ts_test <- window(Median_price_ts, start = c(2019,1), end = c(2019,12), frequency = 12)

# Split train and test data for Average rate
Average_rate_ts_train <- ts(chicago_data$Average_rate, start = c(2008, 2), end = c(2014, 12), frequency = 12)
Average_rate_ts_test <- ts(chicago_data$Average_rate, start = c(2015, 1), end = c(2016, 12), frequency = 12)
```

```{r, message = FALSE, warning = FALSE}
# Create a window
train_end <- c(2018, 12)
test_start <- c(2019, 1)
test_end <- c(2019, 12)
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Median_price_ts_train <- window(Median_price_ts, end = train_end)
Median_price_ts_test <- window(Median_price_ts, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Average_rate_ts_train <- window(Average_rate_ts, end = train_end)
Average_rate_ts_test <- window(Average_rate_ts, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Check correlation
cor(Median_price_ts_train, Average_rate_ts_train)
```

### VAR model

#### Stationarity

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation for Media price (train data)
ggAcf(Median_price_ts_train, color = "#1277e1", size = 1) +
         labs(title = "ACF for Median price (train) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation for Median price (train data)
ggPacf(Median_price_ts_train, color = "#1277e1", size = 1) +
         labs(title = "PACF for Median price (train) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation for Average rate (train data)
ggAcf(Average_rate_ts_train, color = "#1277e1", size = 1) +
         labs(title = "ACF for Average rate (train) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation for Average rate (train data)
ggPacf(Average_rate_ts_train, color = "#1277e1", size = 1) +
         labs(title = "PACF for Average rate (train) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

As we can see, we have a strong autocorrelation in both cases. 

#### Tests

However, let;s check it formally with tests. 

This is a slide from our Time Series class with Professor Abado as a reference. 

![](https://i.ibb.co/hK2j5FP/image.png)

```{r, message = FALSE, warning = FALSE}
# Run KPSS test for Median price train data
kpss.test(Median_price_ts_train)
```

```{r, message = FALSE, warning = FALSE}
# Run KPSS test for Average rate train data
kpss.test(Average_rate_ts_train)
```

```{r, message = FALSE, warning = FALSE}
# Run ADF test for Average rate train data
adf.test(Median_price_ts_train)
```

```{r, message = FALSE, warning = FALSE}
# Run ADF test for Average rate train data
adf.test(Average_rate_ts_train)
```

As we can see, both tests for both variables tell us that the process is not stationary. 

#### VAR select

Now we will use `VARselect` and try to find a model.

```{r, message = FALSE, warning = FALSE}
# VARselect
model1 <- VARselect(cbind(Median_price_ts_train, Average_rate_ts_train))

# Print
model1
```

`VARselect` offers us to try model with p = 10

#### Model

```{r, message = FALSE, warning = FALSE}
# Var 10
varmodel10 <- VAR(cbind(Median_price_ts_train, Average_rate_ts_train), p = 10, type = "both", season = 12) 

# Pormonteau test
serial.test(varmodel10, lags.pt = 20, type = "PT.asymptotic") 
# p-value = 0.06771

# Print summary
summary(varmodel10)
```

```{r, message = FALSE, warning = FALSE}
# Check residuals for the model
ggAcf(residuals(varmodel10)[, 1])
```

#### Forecast

```{r, message = FALSE, warning = FALSE}
# Create a forecast
varmodel10_forecast <- ts(predict(varmodel10, n.ahead = 12, ci = .95)$fcst$Median_price_ts_train[, 1], start = test_start, frequency = 12)
```

#### Evaluate model

Now we can check our numbers in different metrics.

```{r, message = FALSE, warning = FALSE}
# Analyze accuracy
accuracy(varmodel10_forecast, Median_price_ts_test)
accuracy(varmodel10_forecast, Average_rate_ts_test)
```

We can see that our key metrics - such as MAE and RMSE are pretty high, hopefully we can achieve better results in our next models. 

#### Cross-Validation

```{r, message = FALSE, warning = FALSE}
# Define parameters
k <- 72 # Minimum observations (6 years)
p <- 12 # Period
st <- 2014 + 11/12
```

```{r, message = FALSE, warning = FALSE}
error <- matrix(NA, 36, 12)
dates <- c()
for(i in (1:36))

# Create a function  
{
  train <- window(Median_price_ts, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts, start = st + (i + 1)/p, end = st+(i + 12)/p)
  dates[i] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 10, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i, 1:length(test)] <- f - test
}

# Define and plot metrics
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l", col = 3, xlab = "Horizon", ylab = "Error", main = 'Cross Validation Results: 1/31/2016 to 12/31/2018') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
error <- matrix(NA, 12, 12)
dates <- c()
for(i in (37:48))
  
{
  train <- window(Median_price_ts, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts, start = st + (i + 1)/p, end = st + (i + 12)/p)
  dates[i - 36] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 10, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i - 36, 1:length(test)] <- f - test
}

# Define metrics and plot them  
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l",col = 3, xlab = "Horizon", ylab = "Error", main = 'Test Set Results: 1/31/2019 to 12/31/2019') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
plot(f)
points(test)
```

We can see that our prediction is not that accurate and we can do better here. 

### VAR with Diff1

**Important:** Here and in all other steps I replicate the first result, so there is a lot of repetition here, which is still important. 

#### 1st order differencing

First, we need to difference data (1st order).

```{r, message = FALSE, warning = FALSE}
# Create new variables
Median_price_ts_diff1 <- diff(Median_price_ts, differences = 1)
Average_rate_ts_diff1 <- diff(Average_rate_ts, differences = 1)

# Check class
class(Median_price_ts_diff1)
class(Average_rate_ts_diff1)

# Print
Median_price_ts_diff1
Average_rate_ts_diff1
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test for Median price
Median_price_ts_train_diff1 <- window(Median_price_ts_diff1, end = train_end)
Median_price_ts_test_diff1 <- window(Median_price_ts_diff1, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test for Average rate
Average_rate_ts_train_diff1 <- window(Average_rate_ts_diff1, end = train_end)
Average_rate_ts_test_diff1 <- window(Average_rate_ts_diff1, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Check correlation between differenced variables
cor(Median_price_ts_train_diff1, Average_rate_ts_train_diff1)
```

We can see that our correltion dropped significantly after differencing - from `0.48` to `0.16`. 

#### Stationarity

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation
ggAcf(Median_price_ts_train_diff1, color = "#1277e1", size = 1) +
         labs(title = "ACF for Median price (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Median_price_ts_train_diff1, color = "#1277e1", size = 1) +
         labs(title = "PACF for Median price (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation
ggAcf(Average_rate_ts_train_diff1, color = "#1277e1", size = 1) +
         labs(title = "ACF for Average rate (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Average_rate_ts_train_diff1, color = "#1277e1", size = 1) +
         labs(title = "PACF for Average rate (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

As we can see, we have a less strong autocorrelation in both cases, but it still exists, especially with Median price.  

#### Tests

```{r, message = FALSE, warning = FALSE}
kpss.test(Median_price_ts_train_diff1)
```

```{r, message = FALSE, warning = FALSE}
kpss.test(Average_rate_ts_train_diff1)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Median_price_ts_train_diff1)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Average_rate_ts_train_diff1)
```

As we can see, both tests for both variables tell us that the process is stationary. 

#### VAR select

```{r, message = FALSE, warning = FALSE}
# Use VAR select
model1_diff1 <- VARselect(cbind(Median_price_ts_train_diff1, Average_rate_ts_train_diff1))

# Print the model
model1_diff1
```

`VARselect` offers us to try model with p = 8.

#### Model

```{r, message = FALSE, warning = FALSE}
# Var 8
varmodel8 <- VAR(cbind(Median_price_ts_train_diff1, Average_rate_ts_train_diff1), p = 8, type = "both", season = 12) 

# Pormonteau test
serial.test(varmodel8, lags.pt = 20, type = "PT.asymptotic") 
``` 

```{r, message = FALSE, warning = FALSE}
# Print
summary(varmodel8)
```

```{r, message = FALSE, warning = FALSE}
# Check residuals for the model
ggAcf(residuals(varmodel8)[, 1])
```

#### Forecast

```{r, message = FALSE, warning = FALSE}
# Create a forecast
varmodel8_forecast <- ts(predict(varmodel8, n.ahead = 12, ci = .95)$fcst$Median_price_ts_train[, 1], start = test_start, frequency = 12)
```

#### Evaluate the model

```{r, message = FALSE, warning = FALSE}
# Analyze accuracy
accuracy(varmodel8_forecast, Median_price_ts_test_diff1)
accuracy(varmodel8_forecast, Average_rate_ts_test_diff1)
```

#### Cross-validation

```{r, message = FALSE, warning = FALSE}
# Define parameters
k <- 72 # Minimum observations (6 years)
p <- 12 # Period
st <- 2014 + 11/12
error <- matrix(NA, 36, 12)
dates <- c()
for(i in (1:36))

# Create a function  
{
  train <- window(Median_price_ts_diff1, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff1, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff1, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff1, start = st + (i + 1)/p, end = st+(i + 12)/p)
  dates[i] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 8, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i, 1:length(test)] <- f - test
}

# Define and plot metrics
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l", col = 3, xlab = "Horizon", ylab = "Error", main = 'Cross Validation Results: 1/31/2016 to 12/31/2018') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
error <- matrix(NA, 12, 12)
dates <- c()
for(i in (37:48))
  
{
  train <- window(Median_price_ts_diff1, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff1, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff1, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff1, start = st + (i + 1)/p, end = st + (i + 12)/p)
  dates[i - 36] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 8, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i - 36, 1:length(test)] <- f - test
}

# Define metrics and plot them  
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l",col = 3, xlab = "Horizon", ylab = "Error", main = 'Test Set Results: 1/31/2019 to 12/31/2019') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
# Compare results
plot(f)
points(test)
```

### VAR with Diff2

#### 2st order differencing

Again, we need to difference data (2st order)

```{r, message = FALSE, warning = FALSE}
# Create new variables
Median_price_ts_diff2 <- diff(Median_price_ts, differences = 2)
Average_rate_ts_diff2 <- diff(Average_rate_ts, differences = 2)

# Check class
class(Median_price_ts_diff2)
class(Average_rate_ts_diff2)

# Print
Median_price_ts_diff2
Average_rate_ts_diff2
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Median_price_ts_train_diff2 <- window(Median_price_ts_diff2, end = train_end)
Median_price_ts_test_diff2 <- window(Median_price_ts_diff2, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Average_rate_ts_train_diff2 <- window(Average_rate_ts_diff2, end = train_end)
Average_rate_ts_test_diff2 <- window(Average_rate_ts_diff2, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Check correlation
cor(Median_price_ts_train_diff2, Average_rate_ts_train_diff2)
```

We can see that our corrlation dropped slightly this time. 

#### Stationarity

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation for Median price
ggAcf(Median_price_ts_train_diff2, color = "#1277e1", size = 1) +
         labs(title = "ACF for Median price (train diff2) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Median_price_ts_train_diff2, color = "#1277e1", size = 1) +
         labs(title = "PACF for Median price (train diff2) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation
ggAcf(Average_rate_ts_train_diff2, color = "#1277e1", size = 1) +
         labs(title = "ACF for Average rate (train diff2) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Average_rate_ts_train_diff2, color = "#1277e1", size = 1) +
         labs(title = "PACF for Average rate (train diff2) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

#### Tests

```{r, message = FALSE, warning = FALSE}
kpss.test(Median_price_ts_train_diff2)
```

```{r, message = FALSE, warning = FALSE}
kpss.test(Average_rate_ts_train_diff2)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Median_price_ts_train_diff2)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Average_rate_ts_train_diff2)
```

#### VAR select

```{r, message = FALSE, warning = FALSE}
# Use VAR select
model1_diff2 <- VARselect(cbind(Median_price_ts_train_diff2, Average_rate_ts_train_diff2))

# Print the model
model1_diff2
```

`VARselect` offers us to try model with p = 10.

#### Model

```{r, message = FALSE, warning = FALSE}
# Var 10
varmodel10_diff2 <- VAR(cbind(Median_price_ts_train_diff2, Average_rate_ts_train_diff2), p = 10, type = "both", season = 12) 

# Pormonteau test
serial.test(varmodel10_diff2, lags.pt = 20, type = "PT.asymptotic") 
``` 

```{r, message = FALSE, warning = FALSE}
summary(varmodel10_diff2)
```

```{r, message = FALSE, warning = FALSE}
# Check residuals for the model
ggAcf(residuals(varmodel10_diff2)[, 1])
```

#### Forecast

```{r, message = FALSE, warning = FALSE}
# Create a forecast
varmodel10_diff2_forecast <- ts(predict(varmodel10_diff2, n.ahead = 12, ci = .95)$fcst$Median_price_ts_train[, 1], start = test_start, frequency = 12)
```

#### Evaluate the model

```{r, message = FALSE, warning = FALSE}
# Analyze accuracy
accuracy(varmodel10_diff2_forecast, Median_price_ts_test_diff2)
accuracy(varmodel10_diff2_forecast, Average_rate_ts_test_diff2)
```

#### Cross-validation

```{r, message = FALSE, warning = FALSE}
# Define parameters
k <- 72 # Minimum observations (6 years)
p <- 12 # Period
st <- 2014 + 11/12
error <- matrix(NA, 36, 12)
dates <- c()
for(i in (1:36))

# Create a function  
{
  train <- window(Median_price_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff2, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff2, start = st + (i + 1)/p, end = st+(i + 12)/p)
  dates[i] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 10, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i, 1:length(test)] <- f - test
}

# Define and plot metrics
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l", col = 3, xlab = "Horizon", ylab = "Error", main = 'Cross Validation Results: 1/31/2016 to 12/31/2018') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
error <- matrix(NA, 12, 12)
dates <- c()
for(i in (37:48))
  
{
  train <- window(Median_price_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff2, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff2, start = st + (i + 1)/p, end = st + (i + 12)/p)
  dates[i - 36] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 8, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i - 36, 1:length(test)] <- f - test
}

# Define metrics and plot them  
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l",col = 3, xlab = "Horizon", ylab = "Error", main = 'Test Set Results: 1/31/2019 to 12/31/2019') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
plot(f)
points(test)
```

### VAR with Diff2-1

#### 2nd and 1st order differencing

Now I will try to apply different levels of differencing for variables. The median price will have the second order and the average rate will have the first one. 

```{r, message = FALSE, warning = FALSE}
# Create new variables
Median_price_ts_diff2 <- diff(Median_price_ts, differences = 2)
Average_rate_ts_diff1 <- diff(Average_rate_ts, differences = 1)

# Check class
class(Median_price_ts_diff2)
class(Average_rate_ts_diff1)

# Print
Median_price_ts_diff2
Average_rate_ts_diff1
```

Now we need to remove one element from `Average_rate_ts_diff1` to make it equal. 

```{r, message = FALSE, warning = FALSE}
# Remove one element
Average_rate_ts_diff1_cut <- window(Average_rate_ts_diff1, start = c(2008, 4))
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Median_price_ts_train_diff2 <- window(Median_price_ts_diff2, end = train_end)
Median_price_ts_test_diff2 <- window(Median_price_ts_diff2, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Define again train and test 
Average_rate_ts_train_diff1_cut <- window(Average_rate_ts_diff1_cut, end = train_end)
Average_rate_ts_test_diff1_cut <- window(Average_rate_ts_diff1_cut, start = test_start, end = test_end)
```

```{r, message = FALSE, warning = FALSE}
# Check correlation
cor(Median_price_ts_train_diff2, Average_rate_ts_train_diff1_cut)
```

We can see that correlation between even less. 

#### Stationarity

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation
ggAcf(Median_price_ts_train_diff2,color = "#1277e1", size = 1) +
         labs(title = "ACF for Median price (train diff2) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Median_price_ts_train_diff2, color = "#1277e1", size = 1) +
         labs(title = "PACF for Median price (train) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check autocorrelation
ggAcf(Average_rate_ts_train_diff1_cut,color = "#1277e1", size = 1) +
         labs(title = "ACF for Average rate (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

```{r, message = FALSE, warning = FALSE}
# Check partial autocorrelation
ggPacf(Average_rate_ts_train_diff1_cut, color = "#1277e1", size = 1) +
         labs(title = "PACF for Median price (train diff1) in Chicago Metro Area", subtitle = "February 2008 - August 2020")
```

As we can see, we have a slightly less strong autocorrelation in both cases. 

#### Tests

```{r, message = FALSE, warning = FALSE}
kpss.test(Median_price_ts_train_diff2)
```

```{r, message = FALSE, warning = FALSE}
kpss.test(Average_rate_ts_train_diff1_cut)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Median_price_ts_train_diff2)
```

```{r, message = FALSE, warning = FALSE}
adf.test(Average_rate_ts_train_diff1_cut)
```

#### VAR select

```{r, message = FALSE, warning = FALSE}
# Use VAR select
model1_diff21 <- VARselect(cbind(Median_price_ts_train_diff2, Average_rate_ts_train_diff1_cut))

# Print the model
model1_diff21
```

`VARselect` offers us to try model with p = 10.

#### Model

```{r, message = FALSE, warning = FALSE}
# Var 10
varmodel10_diff21 <- VAR(cbind(Median_price_ts_train_diff2, Average_rate_ts_train_diff1_cut), p = 10, type = "both", season = 12) 

# Pormonteau test
serial.test(varmodel10_diff21, lags.pt = 20, type = "PT.asymptotic") 
``` 

```{r, message = FALSE, warning = FALSE}
# Print model
summary(varmodel10_diff21)
```

```{r, message = FALSE, warning = FALSE}
# Check residuals for the model
ggAcf(residuals(varmodel10_diff21)[, 1])
```

#### Forecast

```{r, message = FALSE, warning = FALSE}
# Create a forecast
varmodel10_diff21_forecast <- ts(predict(varmodel10_diff21, n.ahead = 12, ci = .95)$fcst$Median_price_ts_train[, 1], start = test_start, frequency = 12)
```

#### Evaluate the model

```{r, message = FALSE, warning = FALSE}
# Analyze accuracy
accuracy(varmodel10_diff21_forecast, Median_price_ts_test_diff2)
accuracy(varmodel10_diff21_forecast, Average_rate_ts_test_diff1_cut)
```

#### Cross-validation

```{r, message = FALSE, warning = FALSE}
# Define parameters
k <- 72 # Minimum observations (6 years)
p <- 12 # Period
st <- 2014 + 11/12
error <- matrix(NA, 36, 12)
dates <- c()
for(i in (1:36))

# Create a function  
{
  train <- window(Median_price_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff2, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff1_cut, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff1_cut, start = st + (i + 1)/p, end = st+(i + 12)/p)
  dates[i] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 10, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i, 1:length(test)] <- f - test
}

# Define and plot metrics
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l", col = 3, xlab = "Horizon", ylab = "Error", main = 'Cross Validation Results: 1/31/2016 to 12/31/2018') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
error <- matrix(NA, 12, 12)
dates <- c()
for(i in (37:48))
  
{
  train <- window(Median_price_ts_diff2, start = st + (i - k + 1)/p, end = st + i/p)
  test <- window(Median_price_ts_diff2, start = st + (i + 1)/p, end = st + (i + 12)/p)
  rate_train <- window(Average_rate_ts_diff1_cut, start = st + (i - k + 1)/p, end = st + i/p)
  rate_test <- window(Average_rate_ts_diff1_cut, start = st + (i + 1)/p, end = st + (i + 12)/p)
  dates[i - 36] <- st + (i + 12)/p
  var_model <-  VAR(cbind(train, rate_train), p = 8, type = "both", season = 12)
  
  f <- ts(predict(var_model, n.ahead = 12, ci = .95)$fcst$train[, 1], start = st + (i + 1)/p, frequency = 12)
  error[i - 36, 1:length(test)] <- f - test
}

# Define metrics and plot them  
MAE <- colMeans(abs(error), na.rm = TRUE)
RMSE <- sqrt(colMeans(error**2, na.rm = TRUE))
plot(1:12, RMSE, type = "l",col = 3, xlab = "Horizon", ylab = "Error", main = 'Test Set Results: 1/31/2019 to 12/31/2019') 
lines(1:12, MAE, type = "l", col = 2)
legend("topright", legend = c("RMSE", "MAE"), col = 1:4, lty = 1)
```

```{r, message = FALSE, warning = FALSE}
# Print metrics
print(MAE)
print(RMSE)
print(sum(MAE)/12)
print(sum(RMSE)/12)

# Plot residuals
plot(dates, error[, 12], ylab = 'Residual', main = 'Residuals from 12 Month Horizon')
```

```{r, message = FALSE, warning = FALSE}
# Compare
plot(f)
points(test)
```

### Conclusion

We can see that our VAR model 10 with the 1st order of differencing gives us the best results. 

```{r, message = FALSE, warning = FALSE}
# Print accuracy again
accuracy(varmodel8_forecast, Median_price_ts_test_diff1)
accuracy(varmodel8_forecast, Average_rate_ts_test_diff1)
```

THis results are similar to the best results, obtained by other team members in their VAR and ARIMA models. 

### Useful resources

- [StackOverflow - VAR methodology](https://stats.stackexchange.com/questions/191851/var-forecasting-methodology/195477#195477)

- [Forecasting methodology and k-fold cross validation for a vector autoregression](https://stats.stackexchange.com/questions/200598/forecasting-methodology-and-k-fold-cross-validation-for-a-vector-autoregression)

- [Introduction to Econometrics with R - Vector Autoregressions](https://www.econometrics-with-r.org/16-1-vector-autoregressions.html)

- [Forecasting: Principles and Practice](https://otexts.com/fpp3/VAR.html)

- [Penn State - Vector Autoregressive models VAR(p) models](https://online.stat.psu.edu/stat510/lesson/11/11.2)

- [VAR model tutorial](https://rpubs.com/bostonchopsticks/405570)

- [Presentation](http://www.ams.sunysb.edu/~zhu/ams586/VAR_Lecture2.pdf)









